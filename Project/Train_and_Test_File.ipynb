{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, classification_report, log_loss, jaccard_similarity_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read train data (2015 and 2016) and test data (2017 data)\n",
    "df1 = pd.read_csv('df2015_cleaned.csv', index_col = 0)\n",
    "df2 = pd.read_csv('df2016_cleaned.csv', index_col = 0)\n",
    "df_train = pd.concat([df1, df2])\n",
    "df_test = pd.read_csv('df2017_cleaned.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (354313, 14) (354313,)\n",
      "Test set: (165646, 14) (165646,)\n"
     ]
    }
   ],
   "source": [
    "# Train set\n",
    "X = df_train.drop(['P_ISEV'], axis =1).values\n",
    "Y = df_train['P_ISEV'].values\n",
    "#Test set\n",
    "X_test = df_test.drop(['P_ISEV'], axis =1).values\n",
    "Y_test = df_test['P_ISEV'].values\n",
    "print ('Train set:', X.shape,  Y.shape)\n",
    "print ('Test set:', X_test.shape,  Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (283450, 14) (283450,)\n",
      "Validation set: (70863, 14) (70863,)\n"
     ]
    }
   ],
   "source": [
    "# Split the train data into train and validation sets\n",
    "X_train, X_validate, Y_train, Y_validate = train_test_split( X, Y, test_size=0.2, random_state= 4) \n",
    "print ('Train set:', X_train.shape,  Y_train.shape)\n",
    "print ('Validation set:', X_validate.shape,  Y_validate.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "#Train the model\n",
    "MLR = linear_model.LogisticRegression(multi_class='multinomial', solver='newton-cg').fit(X_train, Y_train)\n",
    "\n",
    "#Validate\n",
    "MLR_validate = MLR.predict(X_validate)\n",
    "\n",
    "#Predict \n",
    "MLR_pred =  MLR.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Logistic regression's accuracy_score :  0.6127739440892991\n",
      "Multinomial Logistic regression's f1_score:  0.5966365101973815\n",
      "Multinomial Logistic regression's recall_score:  0.6127739440892991\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.57      0.40      0.47     30257\n",
      "           2       0.63      0.78      0.70     40215\n",
      "           3       0.00      0.00      0.00       391\n",
      "\n",
      "   micro avg       0.61      0.61      0.61     70863\n",
      "   macro avg       0.40      0.39      0.39     70863\n",
      "weighted avg       0.60      0.61      0.60     70863\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check accuracy of validation set\n",
    "print(\"Multinomial Logistic regression's accuracy_score : \", accuracy_score(Y_validate, MLR_validate))\n",
    "print(\"Multinomial Logistic regression's f1_score: \", f1_score(Y_validate, MLR_validate, average = 'weighted'))\n",
    "print(\"Multinomial Logistic regression's recall_score: \", recall_score(Y_validate, MLR_validate, average = 'weighted'))\n",
    "print (classification_report(Y_validate, MLR_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Logistic regression's accuracy_score :  0.6135191915289231\n",
      "Multinomial Logistic regression's f1_score:  0.5976709177333176\n",
      "Multinomial Logistic regression's recall_score:  0.6135191915289231\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.58      0.41      0.48     70849\n",
      "           2       0.63      0.78      0.70     93875\n",
      "           3       0.00      0.00      0.00       922\n",
      "\n",
      "   micro avg       0.61      0.61      0.61    165646\n",
      "   macro avg       0.40      0.39      0.39    165646\n",
      "weighted avg       0.60      0.61      0.60    165646\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check the accuracy of test set\n",
    "print(\"Multinomial Logistic regression's accuracy_score : \", accuracy_score(Y_test, MLR_pred))\n",
    "print(\"Multinomial Logistic regression's f1_score: \", f1_score(Y_test, MLR_pred, average = 'weighted'))\n",
    "print(\"Multinomial Logistic regression's recall_score: \", recall_score(Y_test, MLR_pred, average = 'weighted'))\n",
    "print (classification_report(Y_test, MLR_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss:  0.6767585549502647\n"
     ]
    }
   ],
   "source": [
    "#Log loss of test set\n",
    "MLR_pred_prob = MLR.predict_proba(X_test)\n",
    "print(\"Log Loss: \", log_loss(Y_test, MLR_pred_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# train model\n",
    "RFC = RandomForestClassifier(n_estimators=1000).fit(X_train, Y_train)\n",
    "\n",
    "#Validate\n",
    "RFC_validate = RFC.predict(X_validate)\n",
    "\n",
    "# predict on test set\n",
    "RFC_pred = RFC.predict(X_test)\n",
    "\n",
    "#Check accuracy of validation set\n",
    "print(\"Random Forest Classifier's accuracy_score: \",accuracy_score(Y_validate, RFC_validate))\n",
    "print(\"Random Forest Classifier's f1_score: \",f1_score(Y_validate, RFC_validate, average = 'weighted'))\n",
    "print(\"Random Forest Classifier's recall_score: \",recall_score(Y_validate, RFC_validate, average = 'weighted'))\n",
    "print (classification_report(Y_validate, RFC_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier's accuracy_score:  0.6513951438610048\n",
      "Random Forest Classifier's f1_score:  0.6471731617116983\n",
      "Random Forest Classifier's recall_score:  0.6513951438610048\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.61      0.55      0.58     70849\n",
      "           2       0.68      0.73      0.70     93875\n",
      "           3       0.88      0.03      0.06       922\n",
      "\n",
      "   micro avg       0.65      0.65      0.65    165646\n",
      "   macro avg       0.72      0.44      0.45    165646\n",
      "weighted avg       0.65      0.65      0.65    165646\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check accuracy of test set\n",
    "print(\"Random Forest Classifier's accuracy_score: \",accuracy_score(Y_test, RFC_pred))\n",
    "print(\"Random Forest Classifier's f1_score: \",f1_score(Y_test, RFC_pred, average = 'weighted'))\n",
    "print(\"Random Forest Classifier's recall_score: \",recall_score(Y_test, RFC_pred, average = 'weighted'))\n",
    "print (classification_report(Y_test, RFC_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTrees's accuracy_score:  0.6225392659074552\n",
      "DecisionTrees's f1_score: 0.6226466304249747\n",
      "DecisionTrees's recall_score: 0.6225392659074552\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.55      0.65      0.60     30257\n",
      "           2       0.69      0.61      0.65     40215\n",
      "           3       0.00      0.00      0.00       391\n",
      "\n",
      "   micro avg       0.62      0.62      0.62     70863\n",
      "   macro avg       0.42      0.42      0.42     70863\n",
      "weighted avg       0.63      0.62      0.62     70863\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#train the model\n",
    "DTC = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4).fit(X_train,Y_train)\n",
    "\n",
    "#Validate\n",
    "DTC_validate = DTC.predict(X_validate)\n",
    "\n",
    "#predict on test set\n",
    "DTC_pred = DTC.predict(X_test)\n",
    "\n",
    "#Check accuracy of validation set\n",
    "print(\"DecisionTrees's accuracy_score: \", accuracy_score(Y_validate, DTC_validate))\n",
    "print(\"DecisionTrees's f1_score:\" , f1_score(Y_validate, DTC_validate, average = 'weighted'))\n",
    "print(\"DecisionTrees's recall_score:\" , recall_score(Y_validate, DTC_validate, average = 'weighted'))\n",
    "print (classification_report(Y_validate, DTC_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTrees's accuracy_score:  0.6229730871859266\n",
      "DecisionTrees's f1_score: 0.6230149553744464\n",
      "DecisionTrees's recall_score: 0.6229730871859266\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.55      0.66      0.60     70849\n",
      "           2       0.70      0.60      0.65     93875\n",
      "           3       0.00      0.00      0.00       922\n",
      "\n",
      "   micro avg       0.62      0.62      0.62    165646\n",
      "   macro avg       0.42      0.42      0.42    165646\n",
      "weighted avg       0.63      0.62      0.62    165646\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check accuracy of test set\n",
    "print(\"DecisionTrees's accuracy_score: \", accuracy_score(Y_test, DTC_pred))\n",
    "print(\"DecisionTrees's f1_score:\" , f1_score(Y_test, DTC_pred, average = 'weighted'))\n",
    "print(\"DecisionTrees's recall_score:\" , recall_score(Y_test, DTC_pred, average = 'weighted'))\n",
    "print (classification_report(Y_test, DTC_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbor's Accuracy:  0.6105725131591945\n",
      "K-Nearest Neighbor's f1_score:  0.6039345106887882\n",
      "K-Nearest Neighbor's recall_score:  0.6105725131591945\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.56      0.48      0.52     30257\n",
      "           2       0.64      0.71      0.68     40215\n",
      "           3       0.00      0.00      0.00       391\n",
      "\n",
      "   micro avg       0.61      0.61      0.61     70863\n",
      "   macro avg       0.40      0.40      0.40     70863\n",
      "weighted avg       0.60      0.61      0.60     70863\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "k = 30\n",
    "KNC = KNeighborsClassifier(n_neighbors = k).fit(X_train,Y_train)\n",
    "\n",
    "#Validate\n",
    "KNC_validate = KNC.predict(X_validate)\n",
    "\n",
    "# Predict on test set\n",
    "KNC_pred = KNC.predict(X_test)\n",
    "\n",
    "# Check the accuracy of validation set\n",
    "print(\"K-Nearest Neighbor's Accuracy: \", accuracy_score(Y_validate, KNC_validate))\n",
    "print(\"K-Nearest Neighbor's f1_score: \",f1_score(Y_validate, KNC_validate, average = 'weighted'))\n",
    "print(\"K-Nearest Neighbor's recall_score: \",recall_score(Y_validate, KNC_validate, average = 'weighted'))\n",
    "print (classification_report(Y_validate, KNC_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbor's Accuracy:  0.6061540876326624\n",
      "K-Nearest Neighbor's f1_score:  0.5992118915313438\n",
      "K-Nearest Neighbor's recall_score:  0.6061540876326624\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.55      0.47      0.51     70849\n",
      "           2       0.64      0.71      0.67     93875\n",
      "           3       0.00      0.00      0.00       922\n",
      "\n",
      "   micro avg       0.61      0.61      0.61    165646\n",
      "   macro avg       0.40      0.40      0.39    165646\n",
      "weighted avg       0.60      0.61      0.60    165646\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check the accuracy of test set\n",
    "print(\"K-Nearest Neighbor's Accuracy: \", accuracy_score(Y_test, KNC_pred))\n",
    "print(\"K-Nearest Neighbor's f1_score: \",f1_score(Y_test, KNC_pred, average = 'weighted'))\n",
    "print(\"K-Nearest Neighbor's recall_score: \",recall_score(Y_test, KNC_pred, average = 'weighted'))\n",
    "print (classification_report(Y_test, KNC_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy score for each k \n",
      " {1: 0.587, 3: 0.591, 5: 0.596, 7: 0.6, 10: 0.597, 20: 0.608, 30: 0.612}\n"
     ]
    }
   ],
   "source": [
    "#Check for best k\n",
    "ks = [1, 3, 5, 7, 10, 20, 30]\n",
    "mean_accuracy = {}\n",
    "\n",
    "for k in ks:\n",
    "    acc_score= np.zeros(3)\n",
    "    \n",
    "    for n in range(3): # to generate 3 random train/test split for each k\n",
    "        X1_train, X1_test, Y1_train, Y1_test = train_test_split( X_train, Y_train, test_size=0.2)\n",
    "        # train  and predict\n",
    "        KNC = KNeighborsClassifier(n_neighbors = k).fit(X1_train,Y1_train)\n",
    "        KNC_pred = KNC.predict(X1_test)\n",
    "        # evaluate accuracy\n",
    "        acc_score[n] = accuracy_score(Y1_test, KNC_pred)\n",
    "    \n",
    "    mean_accuracy[k] = round(acc_score.mean(),3)\n",
    "    \n",
    "    \n",
    "print('Mean accuracy score for each k \\n', mean_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy score was  0.612 with k = 30\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "highest = max(mean_accuracy.values())\n",
    "k = {k for k, acc in mean_accuracy.items() if acc == highest}\n",
    "k =re.sub(\"{|}\",\"\" , str(k))\n",
    "print( \"The best accuracy score was \",highest, \"with k =\",k) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=5,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "SGD = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5).fit(X_train, Y_train)   \n",
    "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
    "           early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
    "           l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=5,\n",
    "           n_iter_no_change=5, n_jobs=None, penalty='l2', power_t=0.5,\n",
    "           random_state=None, shuffle=True, tol=0.001,\n",
    "           validation_fraction=0.1, verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validate\n",
    "SGD_validate = SGD.predict(X_validate)\n",
    "\n",
    "#Predict on test set\n",
    "SGD_pred = SGD.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD's Accuracy:  0.5637215472108152\n",
      "SGD's f1_score:  0.4628950644833641\n",
      "SGD's recall_score:  0.5637215472108152\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.52      0.08      0.14     30257\n",
      "           2       0.57      0.93      0.71     40215\n",
      "           3       0.02      0.04      0.03       391\n",
      "\n",
      "   micro avg       0.56      0.56      0.56     70863\n",
      "   macro avg       0.37      0.35      0.29     70863\n",
      "weighted avg       0.55      0.56      0.46     70863\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the accuracy of validation set\n",
    "print(\"SGD's Accuracy: \", accuracy_score(Y_validate, SGD_validate))\n",
    "print(\"SGD's f1_score: \",f1_score(Y_validate, SGD_validate, average = 'weighted'))\n",
    "print(\"SGD's recall_score: \",recall_score(Y_validate, SGD_validate, average = 'weighted'))\n",
    "print (classification_report(Y_validate, SGD_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD's Accuracy:  0.5625852722069956\n",
      "SGD's f1_score:  0.46105180594414075\n",
      "SGD's recall_score:  0.5625852722069956\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.52      0.08      0.14     70849\n",
      "           2       0.57      0.93      0.71     93875\n",
      "           3       0.02      0.03      0.02       922\n",
      "\n",
      "   micro avg       0.56      0.56      0.56    165646\n",
      "   macro avg       0.37      0.35      0.29    165646\n",
      "weighted avg       0.55      0.56      0.46    165646\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check the accuracy of test set\n",
    "print(\"SGD's Accuracy: \", accuracy_score(Y_test, SGD_pred))\n",
    "print(\"SGD's f1_score: \",f1_score(Y_test, SGD_pred, average = 'weighted'))\n",
    "print(\"SGD's recall_score: \",recall_score(Y_test, SGD_pred, average = 'weighted'))\n",
    "print (classification_report(Y_test, SGD_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5680119950608573"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "rbf_feature = RBFSampler(gamma=1, random_state=1)\n",
    "X_features = rbf_feature.fit_transform(X_train)\n",
    "kernel = SGDClassifier(max_iter=5).fit(X_features, Y_train)   \n",
    "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
    "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
    "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=5,\n",
    "       n_iter_no_change=5, n_jobs=None, penalty='l2', power_t=0.5,\n",
    "       random_state=None, shuffle=True, tol=0.001, validation_fraction=0.1,\n",
    "       verbose=0, warm_start=False)\n",
    "kernel.score(X_features, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5667205969356338"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_features = rbf_feature.fit_transform(X_test)\n",
    "kernel.score(X_test_features, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
