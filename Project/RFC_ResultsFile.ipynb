{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: Uncomment last line to create results CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, classification_report, log_loss, jaccard_similarity_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read train data (2015 and 2016) and test data (2017 data)\n",
    "df1 = pd.read_csv('df2015_cleaned.csv', index_col = 0)\n",
    "df2 = pd.read_csv('df2016_cleaned.csv', index_col = 0)\n",
    "df_train = pd.concat([df1, df2])\n",
    "df_test = pd.read_csv('df2017_cleaned.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (354313, 14) (354313,)\n",
      "Test set: (165646, 14) (165646,)\n"
     ]
    }
   ],
   "source": [
    "# Train set\n",
    "X = df_train.drop(['P_ISEV'], axis =1).values\n",
    "Y = df_train['P_ISEV'].values\n",
    "#Test set\n",
    "X_test = df_test.drop(['P_ISEV'], axis =1).values\n",
    "Y_test = df_test['P_ISEV'].values\n",
    "print ('Train set:', X.shape,  Y.shape)\n",
    "print ('Test set:', X_test.shape,  Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (283450, 14) (283450,)\n",
      "Validation set: (70863, 14) (70863,)\n"
     ]
    }
   ],
   "source": [
    "# Split the train data into train and validation sets\n",
    "X_train, X_validate, Y_train, Y_validate = train_test_split( X, Y, test_size=0.2, random_state= 4) \n",
    "print ('Train set:', X_train.shape,  Y_train.shape)\n",
    "print ('Validation set:', X_validate.shape,  Y_validate.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier's accuracy_score:  0.6655941746750773\n",
      "Random Forest Classifier's f1_score:  0.6614079466461286\n",
      "Random Forest Classifier's recall_score:  0.6655941746750773\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.63      0.57      0.60     30257\n",
      "           2       0.69      0.75      0.72     40215\n",
      "           3       0.50      0.02      0.04       391\n",
      "\n",
      "   micro avg       0.67      0.67      0.67     70863\n",
      "   macro avg       0.61      0.44      0.45     70863\n",
      "weighted avg       0.66      0.67      0.66     70863\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# train model\n",
    "RFC = RandomForestClassifier(n_estimators=100).fit(X_train, Y_train)\n",
    "\n",
    "#Validate\n",
    "RFC_validate = RFC.predict(X_validate)\n",
    "\n",
    "# predict on test set\n",
    "RFC_pred = RFC.predict(X_test)\n",
    "\n",
    "#Check accuracy of validation set\n",
    "print(\"Random Forest Classifier's accuracy_score: \",accuracy_score(Y_validate, RFC_validate))\n",
    "print(\"Random Forest Classifier's f1_score: \",f1_score(Y_validate, RFC_validate, average = 'weighted'))\n",
    "print(\"Random Forest Classifier's recall_score: \",recall_score(Y_validate, RFC_validate, average = 'weighted'))\n",
    "print (classification_report(Y_validate, RFC_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier's accuracy_score:  0.6519988409016819\n",
      "Random Forest Classifier's f1_score:  0.6477064043888481\n",
      "Random Forest Classifier's recall_score:  0.6519988409016819\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.61      0.55      0.58     70849\n",
      "           2       0.68      0.73      0.71     93875\n",
      "           3       0.97      0.03      0.06       922\n",
      "\n",
      "   micro avg       0.65      0.65      0.65    165646\n",
      "   macro avg       0.75      0.44      0.45    165646\n",
      "weighted avg       0.65      0.65      0.65    165646\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check accuracy of test set\n",
    "print(\"Random Forest Classifier's accuracy_score: \",accuracy_score(Y_test, RFC_pred))\n",
    "print(\"Random Forest Classifier's f1_score: \",f1_score(Y_test, RFC_pred, average = 'weighted'))\n",
    "print(\"Random Forest Classifier's recall_score: \",recall_score(Y_test, RFC_pred, average = 'weighted'))\n",
    "print (classification_report(Y_test, RFC_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine result of Random Forest with train data and compare result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probability of test set\n",
    "RFC_pred_proba = RFC.predict_proba(X_test)\n",
    "\n",
    "#convert to dataframes\n",
    "df_actual = pd.DataFrame(Y_test, columns = ['Actual P_ISEV'])\n",
    "df_result_pred = pd.DataFrame(RFC_pred, columns = ['Predicted P_ISEV'])\n",
    "df_result_proba = pd.DataFrame(RFC_pred_proba, columns = ['Probability(1)', 'Probability(2)', 'Probability(3)'])\n",
    "df_test1 = df_test.reset_index()\n",
    "df_result = pd.concat([df_actual, df_result_pred, df_result_proba, df_test1], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_result.to_csv('df_result_2017test.csv') #uncomment to create csv file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
