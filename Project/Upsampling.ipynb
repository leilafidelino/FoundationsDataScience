{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, classification_report, log_loss, jaccard_similarity_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('df_train.csv', index_col = 0)\n",
    "df_test = pd.read_csv('df_test.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: (165646, 14) (165646,)\n"
     ]
    }
   ],
   "source": [
    "#Test set\n",
    "X_test = df_test.drop(['P_ISEV'], axis =1).values\n",
    "Y_test = df_test['P_ISEV'].values\n",
    "print ('Test set:', X_test.shape,  Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsample the minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    201218\n",
      "2    201218\n",
      "1    151205\n",
      "Name: P_ISEV, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "X1 = df_train  # orginal train data will be used for resampling (labeled as X1)\n",
    "\n",
    "# separate minority and majority classes\n",
    "no_injury = X1[X1.P_ISEV==1]\n",
    "injury = X1[X1.P_ISEV==2]\n",
    "fatal = X1[X1.P_ISEV==3]\n",
    "\n",
    "# upsample minority\n",
    "fatal_upsampled = resample(fatal,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(injury), # match number in majority class\n",
    "                          random_state=16) # reproducible results\n",
    "\n",
    "# combine majority and upsampled minority\n",
    "upsampled = pd.concat([no_injury, injury, fatal_upsampled])\n",
    "\n",
    "# check new class counts\n",
    "print(upsampled.P_ISEV.value_counts())\n",
    "\n",
    "Y1 = upsampled['P_ISEV'].values\n",
    "X1 = (upsampled.drop('P_ISEV', axis=1)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (442912, 14) (442912,)\n",
      "Validation set: (110729, 14) (110729,)\n"
     ]
    }
   ],
   "source": [
    "# Split the upsampled train data into train and validation sets\n",
    "X1_train, X1_validate, Y1_train, Y1_validate = train_test_split( X1, Y1, test_size=0.2, random_state= 4) \n",
    "print ('Train set:', X1_train.shape,  Y1_train.shape)\n",
    "print ('Validation set:', X1_validate.shape,  Y1_validate.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Log Regression's accuracy_score:  0.5362100262803782\n",
      "Multinomial Log Regression's f1_score:  0.5282353337222929\n",
      "Multinomial Log Regression's recall_score:  0.5362100262803782\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.49      0.35      0.41     30502\n",
      "           2       0.50      0.51      0.50     40023\n",
      "           3       0.59      0.70      0.64     40204\n",
      "\n",
      "   micro avg       0.54      0.54      0.54    110729\n",
      "   macro avg       0.53      0.52      0.52    110729\n",
      "weighted avg       0.53      0.54      0.53    110729\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "#train model\n",
    "upsampledMLR = linear_model.LogisticRegression(multi_class='multinomial', solver='newton-cg').fit(X1_train, Y1_train)\n",
    "\n",
    "#Validate\n",
    "upsampledMLR_validate = upsampledMLR.predict(X1_validate)\n",
    "\n",
    "#Predict on test set\n",
    "upsampledMLR_pred = upsampledMLR.predict(X_test)\n",
    "\n",
    "# Checking accuracy of validation set\n",
    "print(\"Multinomial Log Regression's accuracy_score: \", accuracy_score(Y1_validate, upsampledMLR_validate))\n",
    "print(\"Multinomial Log Regression's f1_score: \", f1_score(Y1_validate, upsampledMLR_validate, average = 'weighted'))\n",
    "print(\"Multinomial Log Regression's recall_score: \", recall_score(Y1_validate, upsampledMLR_validate, average = 'weighted'))\n",
    "print (classification_report(Y1_validate, upsampledMLR_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Log Regression's accuracy_score:  0.44333095879164003\n",
      "Multinomial Log Regression's f1_score:  0.5078452187716705\n",
      "Multinomial Log Regression's recall_score:  0.44333095879164003\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.58      0.36      0.44     70849\n",
      "           2       0.63      0.50      0.56     93875\n",
      "           3       0.01      0.68      0.03       922\n",
      "\n",
      "   micro avg       0.44      0.44      0.44    165646\n",
      "   macro avg       0.41      0.51      0.34    165646\n",
      "weighted avg       0.61      0.44      0.51    165646\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking accuracy of test set\n",
    "print(\"Multinomial Log Regression's accuracy_score: \", accuracy_score(Y_test, upsampledMLR_pred))\n",
    "print(\"Multinomial Log Regression's f1_score: \", f1_score(Y_test, upsampledMLR_pred, average = 'weighted'))\n",
    "print(\"Multinomial Log Regression's recall_score: \", recall_score(Y_test, upsampledMLR_pred, average = 'weighted'))\n",
    "print (classification_report(Y_test, upsampledMLR_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest's accuracy_score:  0.787174091701361\n",
      "Random Forest's f1_score:  0.7853023901473754\n",
      "Random Forest's recall_score:  0.787174091701361\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.63      0.56      0.59     30502\n",
      "           2       0.69      0.75      0.72     40023\n",
      "           3       1.00      1.00      1.00     40204\n",
      "\n",
      "   micro avg       0.79      0.79      0.79    110729\n",
      "   macro avg       0.77      0.77      0.77    110729\n",
      "weighted avg       0.79      0.79      0.79    110729\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# train model\n",
    "upsampledRFC = RandomForestClassifier(n_estimators=100).fit(X1_train, Y1_train)\n",
    "\n",
    "#Validate\n",
    "upsampledRFC_validate = upsampledRFC.predict(X1_validate)\n",
    "\n",
    "# Predict on test set\n",
    "upsampledRFC_pred = upsampledRFC.predict(X_test)\n",
    "\n",
    "# Checking accuracy of validation set\n",
    "print(\"Random Forest's accuracy_score: \", accuracy_score(Y1_validate, upsampledRFC_validate))\n",
    "print(\"Random Forest's f1_score: \", f1_score(Y1_validate, upsampledRFC_validate, average = 'weighted'))\n",
    "print(\"Random Forest's recall_score: \", recall_score(Y1_validate, upsampledRFC_validate, average = 'weighted'))\n",
    "print (classification_report(Y1_validate, upsampledRFC_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest's accuracy_score:  0.6507672989387006\n",
      "Random Forest's f1_score:  0.6463296796200824\n",
      "Random Forest's recall_score:  0.6507672989387006\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.61      0.55      0.58     70849\n",
      "           2       0.68      0.74      0.71     93875\n",
      "           3       0.41      0.04      0.08       922\n",
      "\n",
      "   micro avg       0.65      0.65      0.65    165646\n",
      "   macro avg       0.57      0.44      0.45    165646\n",
      "weighted avg       0.65      0.65      0.65    165646\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking accuracy of test set\n",
    "print(\"Random Forest's accuracy_score: \", accuracy_score(Y_test, upsampledRFC_pred))\n",
    "print(\"Random Forest's f1_score: \", f1_score(Y_test, upsampledRFC_pred, average = 'weighted'))\n",
    "print(\"Random Forest's recall_score: \", recall_score(Y_test, upsampledRFC_pred, average = 'weighted'))\n",
    "print (classification_report(Y_test, upsampledRFC_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTrees's accuracy_score:  0.5396057040161114\n",
      "DecisionTrees's f1_score: 0.5118783145707869\n",
      "DecisionTrees's recall_score: 0.5396057040161114\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.54      0.22      0.31     30502\n",
      "           2       0.49      0.51      0.50     40023\n",
      "           3       0.58      0.81      0.68     40204\n",
      "\n",
      "   micro avg       0.54      0.54      0.54    110729\n",
      "   macro avg       0.54      0.51      0.50    110729\n",
      "weighted avg       0.53      0.54      0.51    110729\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#train the model\n",
    "upsampledDTC = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4).fit(X1_train,Y1_train)\n",
    "\n",
    "#Validate\n",
    "upsampledDTC_validate = upsampledDTC.predict(X1_validate)\n",
    "\n",
    "#Predict on test set\n",
    "upsampledDTC_pred = upsampledDTC.predict(X_test)\n",
    "\n",
    "#Check accuracy of the validation set\n",
    "print(\"DecisionTrees's accuracy_score: \", accuracy_score(Y1_validate, upsampledDTC_validate))\n",
    "print(\"DecisionTrees's f1_score:\" , f1_score(Y1_validate, upsampledDTC_validate, average = 'weighted'))\n",
    "print(\"DecisionTrees's recall_score:\" , recall_score(Y1_validate, upsampledDTC_validate, average = 'weighted'))\n",
    "print (classification_report(Y1_validate, upsampledDTC_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTrees's accuracy_score:  0.38445238641440177\n",
      "DecisionTrees's f1_score: 0.44084400299299115\n",
      "DecisionTrees's recall_score: 0.38445238641440177\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.58      0.22      0.32     70849\n",
      "           2       0.58      0.51      0.54     93875\n",
      "           3       0.01      0.78      0.03       922\n",
      "\n",
      "   micro avg       0.38      0.38      0.38    165646\n",
      "   macro avg       0.39      0.50      0.29    165646\n",
      "weighted avg       0.57      0.38      0.44    165646\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check accuracy of the test set\n",
    "print(\"DecisionTrees's accuracy_score: \", accuracy_score(Y_test, upsampledDTC_pred))\n",
    "print(\"DecisionTrees's f1_score:\" , f1_score(Y_test, upsampledDTC_pred, average = 'weighted'))\n",
    "print(\"DecisionTrees's recall_score:\" , recall_score(Y_test, upsampledDTC_pred, average = 'weighted'))\n",
    "print (classification_report(Y_test, upsampledDTC_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbor's Accuracy:  0.5463216739311544\n",
      "K-Nearest Neighbor's f1_score:  0.5657277457382169\n",
      "K-Nearest Neighbor's recall_score:  0.5463216739311544\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.55      0.46      0.50     70849\n",
      "           2       0.62      0.62      0.62     93875\n",
      "           3       0.02      0.36      0.04       922\n",
      "\n",
      "   micro avg       0.55      0.55      0.55    165646\n",
      "   macro avg       0.40      0.48      0.39    165646\n",
      "weighted avg       0.59      0.55      0.57    165646\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "k = 30\n",
    "upsampledKNC = KNeighborsClassifier(n_neighbors = k).fit(X1_train,Y1_train)\n",
    "\n",
    "# KNC_pred is predicted Y\n",
    "upsampledKNC_pred = upsampledKNC.predict(X_test)\n",
    "\n",
    "#Check accuracy of test set\n",
    "print(\"K-Nearest Neighbor's Accuracy: \", accuracy_score(Y_test, upsampledKNC_pred))\n",
    "print(\"K-Nearest Neighbor's f1_score: \",f1_score(Y_test, upsampledKNC_pred, average = 'weighted'))\n",
    "print(\"K-Nearest Neighbor's recall_score: \",recall_score(Y_test, upsampledKNC_pred, average = 'weighted'))\n",
    "print (classification_report(Y_test, upsampledKNC_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=5,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "SGD = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5).fit(X1_train, Y1_train)   \n",
    "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
    "           early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
    "           l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=5,\n",
    "           n_iter_no_change=5, n_jobs=None, penalty='l2', power_t=0.5,\n",
    "           random_state=None, shuffle=True, tol=0.001,\n",
    "           validation_fraction=0.1, verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD's Accuracy:  0.5006547516910655\n",
      "SGD's f1_score:  0.46146249674470013\n",
      "SGD's recall_score:  0.5006547516910655\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.11      0.18     30502\n",
      "           2       0.44      0.72      0.55     40023\n",
      "           3       0.59      0.58      0.58     40204\n",
      "\n",
      "   micro avg       0.50      0.50      0.50    110729\n",
      "   macro avg       0.51      0.47      0.44    110729\n",
      "weighted avg       0.52      0.50      0.46    110729\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Validate\n",
    "upsampledSGD_validate = SGD.predict(X1_validate)\n",
    "\n",
    "# Check the accuracy of validation set\n",
    "print(\"SGD's Accuracy: \", accuracy_score(Y1_validate, upsampledSGD_validate))\n",
    "print(\"SGD's f1_score: \",f1_score(Y1_validate, upsampledSGD_validate, average = 'weighted'))\n",
    "print(\"SGD's recall_score: \",recall_score(Y1_validate, upsampledSGD_validate, average = 'weighted'))\n",
    "print (classification_report(Y1_validate, upsampledSGD_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD's Accuracy:  0.4571435470823322\n",
      "SGD's f1_score:  0.44622725093443916\n",
      "SGD's recall_score:  0.4571435470823322\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.60      0.11      0.19     70849\n",
      "           2       0.59      0.72      0.65     93875\n",
      "           3       0.01      0.55      0.03       922\n",
      "\n",
      "   micro avg       0.46      0.46      0.46    165646\n",
      "   macro avg       0.40      0.46      0.29    165646\n",
      "weighted avg       0.59      0.46      0.45    165646\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predict on test set\n",
    "upsampledSGD_pred = SGD.predict(X_test)\n",
    "\n",
    "#Check accuracy of test set\n",
    "print(\"SGD's Accuracy: \", accuracy_score(Y_test, upsampledSGD_pred))\n",
    "print(\"SGD's f1_score: \",f1_score(Y_test, upsampledSGD_pred, average = 'weighted'))\n",
    "print(\"SGD's recall_score: \",recall_score(Y_test, upsampledSGD_pred, average = 'weighted'))\n",
    "print (classification_report(Y_test, upsampledSGD_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
